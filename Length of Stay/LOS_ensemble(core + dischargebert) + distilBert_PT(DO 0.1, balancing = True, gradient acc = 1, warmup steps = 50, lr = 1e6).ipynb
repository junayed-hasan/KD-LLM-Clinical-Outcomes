{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train, Validation, Test Dataset\n",
    "train_dataset = pd.read_csv('saveDir/LOS_WEEKS_adm_train.csv')\n",
    "val_dataset = pd.read_csv('saveDir/LOS_WEEKS_adm_val.csv')\n",
    "test_dataset = pd.read_csv('saveDir/LOS_WEEKS_adm_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output1 = self.model1(input_ids, attention_mask=attention_mask)[0]\n",
    "        output2 = self.model2(input_ids, attention_mask=attention_mask)[0]\n",
    "        avg_output = (output1 + output2) / 2.00\n",
    "        return avg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bvanaken/CORe-clinical-outcome-biobert-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "# Specify the dropout rate in the configuration\n",
    "config = AutoConfig.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1', \n",
    "                                    num_labels=4, \n",
    "                                    hidden_dropout_prob=0.2, \n",
    "                                    attention_probs_dropout_prob=0.2)\n",
    "\n",
    "# Load the pre-trained model with the specified configuration\n",
    "core_model = AutoModelForSequenceClassification.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1', config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "# # Specify the dropout rate in the configuration\n",
    "# config = AutoConfig.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', \n",
    "#                                     num_labels=4, \n",
    "#                                     hidden_dropout_prob=0.2, \n",
    "#                                     attention_probs_dropout_prob=0.2)\n",
    "\n",
    "# # Load the pre-trained model with the specified configuration\n",
    "# clinical_model = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "# # Specify the dropout rate in the configuration\n",
    "# config = AutoConfig.from_pretrained('dmis-lab/biobert-base-cased-v1.2', \n",
    "#                                     num_labels=4, \n",
    "#                                     hidden_dropout_prob=0.2, \n",
    "#                                     attention_probs_dropout_prob=0.2)\n",
    "\n",
    "# # Load the pre-trained model with the specified configuration\n",
    "# biobert_base_model = AutoModelForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_Discharge_Summary_BERT were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_Discharge_Summary_BERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "# Specify the dropout rate in the configuration\n",
    "config = AutoConfig.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT', \n",
    "                                    num_labels=4, \n",
    "                                    hidden_dropout_prob=0.2, \n",
    "                                    attention_probs_dropout_prob=0.2)\n",
    "\n",
    "# Load the pre-trained model with the specified configuration\n",
    "discharge_model = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Choose a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the training, validation, and test datasets\n",
    "train_encodings = tokenizer(train_dataset['text'].tolist(), truncation=True, padding=True, max_length = 512)\n",
    "val_encodings = tokenizer(val_dataset['text'].tolist(), truncation=True, padding=True,  max_length = 512)\n",
    "test_encodings = tokenizer(test_dataset['text'].tolist(), truncation=True, padding=True , max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset for PyTorch\n",
    "class LosDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LosDataset(train_encodings, train_dataset['los_label'].tolist())\n",
    "val_dataset = LosDataset(val_encodings, val_dataset['los_label'].tolist())\n",
    "test_dataset = LosDataset(test_encodings, test_dataset['los_label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble_model = EnsembleModel(core_model, discharge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found models starting with 'CORE_ensemble(core + dischargebert)':\n",
      "CORE_ensemble(core + dischargebert)_epoch_5roc_0.7133395365724405.pth\n",
      "Loaded Model: CORE_ensemble(core + dischargebert)_epoch_5roc_0.7133395365724405.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# list all files in the current directory\n",
    "files = os.listdir('.')\n",
    "\n",
    "# filter the ones that start with 'CORE_baseline'\n",
    "core_models = [f for f in files if f.startswith('CORE_ensemble(core + dischargebert)')]\n",
    "\n",
    "if core_models:\n",
    "    print(\"Found models starting with 'CORE_ensemble(core + dischargebert)':\")\n",
    "    for model in core_models:\n",
    "        print(model)\n",
    "        \n",
    "    # get the first (and supposedly only) model\n",
    "    model_path = core_models[0]\n",
    "\n",
    "    # load the model state\n",
    "    ensemble_model.load_state_dict(torch.load(model_path))\n",
    "    print(f\"Loaded Model: {model_path}\")\n",
    "else:\n",
    "    print(\"No models found starting with 'CORE_ensemble(core + dischargebert)'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model = ensemble_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# create a student model\n",
    "student_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', \n",
    "                                                  num_labels=4, \n",
    "                                                  hidden_dropout_prob=0.2, \n",
    "                                                  attention_probs_dropout_prob=0.2)\n",
    "\n",
    "student_model = DistilBertForSequenceClassification(student_config)\n",
    "\n",
    "# set the temperature\n",
    "temperature = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No models found starting with 'CORE_ensemble(core + dischargebert) + distilBert'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# list all files in the current directory\n",
    "files = os.listdir('.')\n",
    "\n",
    "# filter the ones that start with 'CORE_baseline'\n",
    "core_models = [f for f in files if f.startswith('CORE_ensemble(core + dischargebert) + distilBert')]\n",
    "\n",
    "if core_models:\n",
    "    print(\"Found models starting with 'CORE_ensemble(core + dischargebert) + distilBert':\")\n",
    "    for model in core_models:\n",
    "        print(model)\n",
    "        \n",
    "    # get the first (and supposedly only) model\n",
    "    model_path = core_models[0]\n",
    "\n",
    "    # load the model state\n",
    "    student_model.load_state_dict(torch.load(model_path))\n",
    "    print(f\"Loaded Model: {model_path}\")\n",
    "else:\n",
    "    print(\"No models found starting with 'CORE_ensemble(core + dischargebert) + distilBert'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model = student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "best_roc_auc = 0.0\n",
    "min_delta = 0.0001\n",
    "early_stopping_count = 0\n",
    "early_stopping_patience = 3\n",
    "gradient_accumulation_steps = 10\n",
    "\n",
    "# Set the optimizer\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Set the scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=50, \n",
    "    num_training_steps=len(train_loader) * epochs // gradient_accumulation_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:05<00:00,  3.41s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:10<00:00,  1.94it/s]\n",
      "/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Training Loss: 1.3936300123527599, Validation Loss: 1.3139223274977312\n",
      "Accuracy: 0.3664313368253245, Recall: 0.3664313368253245, Precision: 0.13427192460759443, F1: 0.19652934031731573, Micro F1: 0.3664313368253245, Macro Roc Auc: 0.5668297955270275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:24<00:00,  3.43s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:11<00:00,  1.93it/s]\n",
      "/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/200, Training Loss: 1.3794044772658565, Validation Loss: 1.3001905852469846\n",
      "Accuracy: 0.3675700296060123, Recall: 0.3675700296060123, Precision: 0.37914853979803653, F1: 0.20316625017938786, Micro F1: 0.3675700296060123, Macro Roc Auc: 0.5800700965664007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:19<00:00,  3.43s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:07<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/200, Training Loss: 1.3534808536182568, Validation Loss: 1.2712859960569851\n",
      "Accuracy: 0.38077886586199045, Recall: 0.38077886586199045, Precision: 0.3681338027605608, F1: 0.31179573771520347, Micro F1: 0.38077886586199045, Macro Roc Auc: 0.6192753537713008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:39<00:00,  3.45s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:11<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/200, Training Loss: 1.3041345427590338, Validation Loss: 1.2484596069308296\n",
      "Accuracy: 0.3919380551127306, Recall: 0.3919380551127306, Precision: 0.38651102962746503, F1: 0.3690170545169432, Micro F1: 0.3919380551127306, Macro Roc Auc: 0.6549495494721381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:13<00:00,  3.42s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:11<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/200, Training Loss: 1.2627145069505388, Validation Loss: 1.2247694488884746\n",
      "Accuracy: 0.40696879981780915, Recall: 0.40696879981780915, Precision: 0.405096903117529, F1: 0.3735218897803632, Micro F1: 0.40696879981780915, Macro Roc Auc: 0.6693954865383643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:08<00:00,  3.42s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:11<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/200, Training Loss: 1.2387308366165803, Validation Loss: 1.2311573952868364\n",
      "Accuracy: 0.4108403552721476, Recall: 0.4108403552721476, Precision: 0.41103383833754353, F1: 0.3892569997420046, Micro F1: 0.4108403552721476, Macro Roc Auc: 0.6794620713120789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [55:08<00:00,  3.48s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:05<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/200, Training Loss: 1.2203472957375423, Validation Loss: 1.2181485373040903\n",
      "Accuracy: 0.42040537462992483, Recall: 0.42040537462992483, Precision: 0.4247213362919185, F1: 0.3859282401896057, Micro F1: 0.42040537462992483, Macro Roc Auc: 0.6838770006917363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:25<00:00,  3.43s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:11<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/200, Training Loss: 1.212052574919852, Validation Loss: 1.214036452597466\n",
      "Accuracy: 0.42199954452288774, Recall: 0.42199954452288774, Precision: 0.42060472079129774, F1: 0.3908388793612484, Micro F1: 0.4219995445228878, Macro Roc Auc: 0.6899074809243989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:33<00:00,  3.44s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:11<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/200, Training Loss: 1.1982120798840006, Validation Loss: 1.2045108637084132\n",
      "Accuracy: 0.4292871783192895, Recall: 0.4292871783192895, Precision: 0.4346382649793847, F1: 0.40518699110547624, Micro F1: 0.4292871783192895, Macro Roc Auc: 0.6886542695294741\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:45<00:00,  3.45s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:12<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/200, Training Loss: 1.1824384224527642, Validation Loss: 1.203469338192456\n",
      "Accuracy: 0.4270097927579139, Recall: 0.4270097927579139, Precision: 0.4350474504812953, F1: 0.4110667956156379, Micro F1: 0.4270097927579139, Macro Roc Auc: 0.6904037278750308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [55:27<00:00,  3.50s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:12<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/200, Training Loss: 1.175224215199644, Validation Loss: 1.2135251993718355\n",
      "Accuracy: 0.4151673878387611, Recall: 0.4151673878387611, Precision: 0.43343990813419997, F1: 0.40531798765823374, Micro F1: 0.4151673878387611, Macro Roc Auc: 0.6900092889601988\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:39<00:00,  3.45s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:12<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/200, Training Loss: 1.168273323091172, Validation Loss: 1.1972021089083906\n",
      "Accuracy: 0.4281484855386017, Recall: 0.4281484855386017, Precision: 0.4237239704426197, F1: 0.41864876391570366, Micro F1: 0.4281484855386017, Macro Roc Auc: 0.6966396165203791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [53:53<00:00,  3.40s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:05<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/200, Training Loss: 1.1601959469441234, Validation Loss: 1.2195514254811881\n",
      "Accuracy: 0.4135732179457982, Recall: 0.4135732179457982, Precision: 0.40802623666665877, F1: 0.3925015600618677, Micro F1: 0.4135732179457982, Macro Roc Auc: 0.6974187591483713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [52:59<00:00,  3.34s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:10<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/200, Training Loss: 1.1598734786708522, Validation Loss: 1.2404665104720904\n",
      "Accuracy: 0.39717604190389433, Recall: 0.39717604190389433, Precision: 0.42862070999538493, F1: 0.38520385407182295, Micro F1: 0.39717604190389433, Macro Roc Auc: 0.6865784149869573\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [53:24<00:00,  3.37s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:04<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/200, Training Loss: 1.1516084192177725, Validation Loss: 1.2137463887532551\n",
      "Accuracy: 0.42199954452288774, Recall: 0.42199954452288774, Precision: 0.4156814625580146, F1: 0.39927619605340564, Micro F1: 0.4219995445228878, Macro Roc Auc: 0.6966672025889888\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/951 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/new_env/lib/python3.9/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 951/951 [54:29<00:00,  3.44s/it]\n",
      "100%|█████████████████████████████████████████| 138/138 [01:12<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/200, Training Loss: 1.1420242595246413, Validation Loss: 1.2083302097044128\n",
      "Accuracy: 0.4260988385333637, Recall: 0.4260988385333637, Precision: 0.42124606163227657, F1: 0.41462073336596816, Micro F1: 0.4260988385333637, Macro Roc Auc: 0.696985810236773\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "ensemble_model.eval()\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    student_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad() if step % gradient_accumulation_steps == 0 else None\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # get student model's logits\n",
    "        student_logits = student_model(input_ids, attention_mask)[0]\n",
    "        \n",
    "        # get teacher model's logits\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = ensemble_model(input_ids, attention_mask)\n",
    "            \n",
    "            \n",
    "        # calculate loss\n",
    "        loss = (\n",
    "            nn.KLDivLoss()(F.log_softmax(student_logits/temperature, dim=1), \n",
    "                           F.softmax(teacher_logits/temperature, dim=1)) * (temperature ** 2) +\n",
    "            nn.CrossEntropyLoss()(student_logits, labels)\n",
    "        )\n",
    "        \n",
    "        (loss / gradient_accumulation_steps).backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    student_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = student_model(input_ids, attention_mask)[0]\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "            val_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_labels = np.concatenate(val_labels)\n",
    "    val_loss /= len(val_loader)\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f'Epoch: {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_preds_class = np.argmax(val_preds, axis=1)\n",
    "    accuracy = accuracy_score(val_labels, val_preds_class)\n",
    "    recall = recall_score(val_labels, val_preds_class, average='weighted')\n",
    "    precision = precision_score(val_labels, val_preds_class, average='weighted')\n",
    "    f1 = f1_score(val_labels, val_preds_class, average='weighted')\n",
    "    micro_f1 = f1_score(val_labels, val_preds_class, average='micro')\n",
    "    macro_roc_auc = roc_auc_score(val_labels, val_preds, multi_class='ovo', average='macro')\n",
    "\n",
    "    print(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')\n",
    "        \n",
    "        \n",
    "    # Implement early stopping\n",
    "    if macro_roc_auc - best_roc_auc < min_delta:\n",
    "        early_stopping_count += 1\n",
    "        print(f'EarlyStopping counter: {early_stopping_count} out of {early_stopping_patience}')\n",
    "        if early_stopping_count >= early_stopping_patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    else:\n",
    "        best_roc_auc = macro_roc_auc\n",
    "        early_stopping_count = 0\n",
    "        torch.save(student_model.state_dict(), f\"CORE_ensemble(core + dischargebert) + distilBert_epoch_{epoch}roc_{best_roc_auc}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found models starting with 'CORE_ensemble(core + dischargebert) + distilBert':\n",
      "CORE_ensemble(core + dischargebert) + distilBert_epoch_12roc_0.6974187591483713.pth\n",
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# list all files in the current directory\n",
    "files = os.listdir('.')\n",
    "\n",
    "# filter the ones that start with 'CORE_baseline'\n",
    "core_models = [f for f in files if f.startswith('CORE_ensemble(core + dischargebert) + distilBert')]\n",
    "\n",
    "if core_models:\n",
    "    print(\"Found models starting with 'CORE_ensemble(core + dischargebert) + distilBert':\")\n",
    "    for model in core_models:\n",
    "        print(model)\n",
    "        \n",
    "    # get the first (and supposedly only) model\n",
    "    model_path = core_models[0]\n",
    "\n",
    "    # load the model state\n",
    "    student_model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Loaded Model\")\n",
    "else:\n",
    "    print(\"No models found starting with 'CORE_ensemble(core + dischargebert) + distilBert'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 275/275 [02:20<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "student_model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "# Iterate over test data\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = student_model(input_ids, attention_mask)[0]\n",
    "        test_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4265090371717631, Recall: 0.4265090371717631, Precision: 0.42588944607031426, F1: 0.4072445736479922, Micro F1: 0.4265090371717631, Macro Roc Auc: 0.7055687966725189\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.concatenate(test_preds)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "test_preds_class = np.argmax(test_preds, axis=1)\n",
    "accuracy = accuracy_score(test_labels, test_preds_class)\n",
    "recall = recall_score(test_labels, test_preds_class, average='weighted')\n",
    "precision = precision_score(test_labels, test_preds_class, average='weighted')\n",
    "f1 = f1_score(test_labels, test_preds_class, average='weighted')\n",
    "micro_f1 = f1_score(test_labels, test_preds_class, average='micro')\n",
    "macro_roc_auc = roc_auc_score(test_labels, test_preds, multi_class='ovo', average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_new_env)",
   "language": "python",
   "name": "conda_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
